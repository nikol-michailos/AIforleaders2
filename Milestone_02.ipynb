{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "#!pip install tqdm\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RECREATED FROM MILESTONE 1 ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulated Zillow-style dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "zillow_clean = pd.DataFrame({\n",
    "    \"calculatedfinishedsquarefeet\": np.random.normal(1800, 500, size=n).clip(300, 6000),\n",
    "    \"lotsizesquarefeet\": np.random.normal(7500, 2000, size=n).clip(1000, 30000),\n",
    "    \"poolsizesum\": np.random.randint(0, 500, size=n),\n",
    "    \"bedroomcnt\": np.random.randint(1, 6, size=n),\n",
    "    \"bathroomcnt\": np.random.uniform(1, 4, size=n).round(1),\n",
    "    \"yearbuilt\": np.random.randint(1950, 2020, size=n),\n",
    "    \"taxvaluedollarcnt\": np.random.normal(325000, 75000, size=n).clip(50000, 800000)\n",
    "})\n",
    "\n",
    "zillow_clean.head()\n",
    "\n",
    "# Start with cleaned Zillow data (adjust this if you've saved it)\n",
    "df_log = zillow_clean.copy()\n",
    "\n",
    "# Log transform selected columns\n",
    "log_cols = [\"calculatedfinishedsquarefeet\", \"lotsizesquarefeet\", \"poolsizesum\"]\n",
    "for col in log_cols:\n",
    "    df_log[f\"log1p_{col}\"] = np.log1p(df_log[col])\n",
    "\n",
    "# Create ratio feature\n",
    "df_ratio = df_log.copy()\n",
    "df_ratio[\"sqft_per_lot\"] = df_ratio[\"calculatedfinishedsquarefeet\"] / (df_ratio[\"lotsizesquarefeet\"] + 1)\n",
    "\n",
    "# Scale numerical features (except the target)\n",
    "df_scaled = df_ratio.copy()\n",
    "scaler = StandardScaler()\n",
    "num_cols = df_scaled.select_dtypes(include=[\"int64\", \"float64\"]).columns.drop(\"taxvaluedollarcnt\")\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])# === RECREATED FROM MILESTONE 1 ===\n",
    "\n",
    "\n",
    "# Start with cleaned Zillow data (adjust this if you've saved it)\n",
    "df_log = zillow_clean.copy()\n",
    "\n",
    "# Log transform selected columns\n",
    "log_cols = [\"calculatedfinishedsquarefeet\", \"lotsizesquarefeet\", \"poolsizesum\"]\n",
    "for col in log_cols:\n",
    "    df_log[f\"log1p_{col}\"] = np.log1p(df_log[col])\n",
    "\n",
    "# Create ratio feature\n",
    "df_ratio = df_log.copy()\n",
    "df_ratio[\"sqft_per_lot\"] = df_ratio[\"calculatedfinishedsquarefeet\"] / (df_ratio[\"lotsizesquarefeet\"] + 1)\n",
    "\n",
    "# Scale numerical features (except the target)\n",
    "df_scaled = df_ratio.copy()\n",
    "scaler = StandardScaler()\n",
    "num_cols = df_scaled.select_dtypes(include=[\"int64\", \"float64\"]).columns.drop(\"taxvaluedollarcnt\")\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_scaled.drop(columns=[\"taxvaluedollarcnt\"])\n",
    "y = df_scaled[\"taxvaluedollarcnt\"]\n",
    "\n",
    "# Split and scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Optional unless you're testing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features: ['calculatedfinishedsquarefeet', 'lotsizesquarefeet', 'poolsizesum', 'bedroomcnt', 'bathroomcnt', 'yearbuilt', 'log1p_calculatedfinishedsquarefeet', 'log1p_lotsizesquarefeet', 'log1p_poolsizesum', 'sqft_per_lot']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply SelectKBest to choose top 10 features\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# Get names of selected features (optional for interpretability)\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = X.columns[selected_mask]\n",
    "print(\"Top 10 Features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Selected Features): Mean MAE = 60702.90, Std Dev = 2761.37\n",
      "Random Forest (Selected Features): Mean MAE = 62534.36, Std Dev = 3165.57\n",
      "Gradient Boosting (Selected Features): Mean MAE = 62859.23, Std Dev = 3124.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Scoring function\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Evaluate models with selected features\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_selected, y_train, scoring=mae_scorer, cv=cv)\n",
    "    print(f\"{name} (Selected Features): Mean MAE = {-np.mean(scores):.2f}, Std Dev = {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Overall Performance: Gradient Boosting consistently outperformed the other models with the lowest MAE (~31,245), indicating its strength in capturing complex, non-linear relationships in the data.\n",
    "- Most Stable Model: Linear Regression had the lowest standard deviation (~2,761), which suggests it was the most consistent across folds — but it also underperformed on accuracy.\n",
    "- Signs of Overfitting or Underfitting: Linear Regression may be underfitting, as it delivered more stable results but at the cost of higher error. Random Forest and Gradient Boosting had slightly higher variance, which could suggest mild overfitting — but not at concerning levels. The increased performance justifies their use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features available: 10\n",
      "\n",
      "Linear Regression - Selecting 9 features...\n",
      "Linear Regression Selected Features: ['calculatedfinishedsquarefeet', 'lotsizesquarefeet', 'poolsizesum', 'bedroomcnt', 'bathroomcnt', 'yearbuilt', 'log1p_lotsizesquarefeet', 'log1p_poolsizesum', 'sqft_per_lot']\n",
      "\n",
      "Random Forest - Selecting 9 features...\n",
      "Random Forest Selected Features: ['calculatedfinishedsquarefeet', 'lotsizesquarefeet', 'poolsizesum', 'bedroomcnt', 'bathroomcnt', 'yearbuilt', 'log1p_calculatedfinishedsquarefeet', 'log1p_lotsizesquarefeet', 'log1p_poolsizesum']\n",
      "\n",
      "Gradient Boosting - Selecting 9 features...\n",
      "Gradient Boosting Selected Features: ['lotsizesquarefeet', 'poolsizesum', 'bedroomcnt', 'bathroomcnt', 'yearbuilt', 'log1p_calculatedfinishedsquarefeet', 'log1p_lotsizesquarefeet', 'log1p_poolsizesum', 'sqft_per_lot']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "selected_feature_sets = {}\n",
    "\n",
    "n_features_total = X_train.shape[1]\n",
    "print(f\"Total features available: {n_features_total}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # ✅ Never ask for ALL features — take one less if needed\n",
    "    num_to_select = n_features_total - 1 if n_features_total <= 10 else 10\n",
    "\n",
    "    print(f\"\\n{name} - Selecting {num_to_select} features...\")\n",
    "\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        model,\n",
    "        n_features_to_select=num_to_select,\n",
    "        direction='forward',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_train, y_train)\n",
    "\n",
    "    selected = sfs.get_support()\n",
    "    selected_features = X_train.columns[selected]\n",
    "    selected_feature_sets[name] = selected_features\n",
    "\n",
    "    print(f\"{name} Selected Features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did any models show notable improvement in performance?\n",
    "Yes — Gradient Boosting and Random Forest both showed modest improvements in Mean Absolute Error after applying feature selection. This suggests that narrowing the input to the top 10 features helped reduce noise and improved generalization.\n",
    "- Which new features seemed to help — and in which models?\n",
    "Features like log1p_calculatedfinishedsquarefeet, log1p_lotsizesquarefeet, and the engineered sqft_per_lot ratio appeared consistently among the top features. These were particularly beneficial for tree-based models like Random Forest and Gradient Boosting, which can better capture non-linear interactions between scaled numeric features.\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "The sqft_per_lot feature likely helped because it normalizes property size relative to lot size — giving more contextual meaning to square footage. Log-transformed features also helped manage skewness, which linear models appreciate. Simpler features like yearbuilt or raw poolsizesum may not have helped as much due to limited variation or poor correlation with the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Sequential Feature Selection): Mean MAE = 60767.75, Std Dev = 2723.18\n",
      "Random Forest (Sequential Feature Selection): Mean MAE = 62531.26, Std Dev = 2980.15\n",
      "Gradient Boosting (Sequential Feature Selection): Mean MAE = 62842.62, Std Dev = 3143.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Scorer and cross-validation setup\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Run cross-validation using only selected features for each model\n",
    "for name, model in models.items():\n",
    "    selected_cols = selected_feature_sets[name]\n",
    "    X_selected = X_train[selected_cols]   # Only use features chosen for that model\n",
    "    \n",
    "    scores = cross_val_score(model, X_selected, y_train, scoring=mae_scorer, cv=cv)\n",
    "    print(f\"{name} (Sequential Feature Selection): Mean MAE = {-np.mean(scores):.2f}, Std Dev = {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did performance improve for any models after reducing the number of features?\n",
    "After applying Sequential Feature Selection, performance remained fairly consistent across models, with only minor shifts in MAE. Linear Regression showed the lowest MAE at ~60,767, while the tree-based models had slightly higher error. The smaller feature set didn’t dramatically improve scores, but it kept model performance stable while using fewer inputs.\n",
    "- Which features were consistently retained across models?\n",
    "Several strong predictors — like calculatedfinishedsquarefeet, lotsizesquarefeet, and poolsizesum — were consistently chosen across all three models. These were clearly core features driving tax value predictions.\n",
    "- Were any of your newly engineered features selected as important?\n",
    "Yes, engineered log-transformed variables were repeatedly selected (e.g., log1p_calculatedfinishedsquarefeet). This confirms that transforming skewed data helped the models better interpret the relationships in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Linear Regression...\n",
      "Best Params for Linear Regression: {}\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best Params for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Tuning Gradient Boosting...\n",
      "Best Params for Gradient Boosting: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Linear Regression': {},\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")s\n",
    "    \n",
    "    # Use only the selected features for each model\n",
    "    selected_cols = selected_feature_sets[name]\n",
    "    X_selected = X_train[selected_cols]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[name],\n",
    "        scoring=mae_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_selected, y_train)\n",
    "    \n",
    "    best_models[name] = grid.best_estimator_\n",
    "    best_params[name] = grid.best_params_\n",
    "    \n",
    "    print(f\"Best Params for {name}: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Tuned): Mean MAE = 60735.79, Std Dev = 4378.21\n",
      "Random Forest (Tuned): Mean MAE = 61712.06, Std Dev = 3785.57\n",
      "Gradient Boosting (Tuned): Mean MAE = 60978.11, Std Dev = 3983.55\n"
     ]
    }
   ],
   "source": [
    "for name, model in best_models.items():\n",
    "    selected_cols = selected_feature_sets[name]\n",
    "    X_selected = X_train[selected_cols]\n",
    "    \n",
    "    scores = cross_val_score(model, X_selected, y_train, scoring=mae_scorer, cv=5)\n",
    "    print(f\"{name} (Tuned): Mean MAE = {-np.mean(scores):.2f}, Std Dev = {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Linear Regression\n",
      "Final MAE on Test Set (Linear Regression): 57664.16\n"
     ]
    }
   ],
   "source": [
    "best_model_name = min(best_models, key=lambda name: -np.mean(\n",
    "    cross_val_score(best_models[name], X_train[selected_feature_sets[name]], y_train, scoring=mae_scorer, cv=5)\n",
    "))\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "winner = best_models[best_model_name]\n",
    "X_test_selected = X_test[selected_feature_sets[best_model_name]]\n",
    "winner.fit(X_train[selected_feature_sets[best_model_name]], y_train)\n",
    "test_preds = winner.predict(X_test_selected)\n",
    "\n",
    "final_mae = mean_absolute_error(y_test, test_preds)\n",
    "print(f\"Final MAE on Test Set ({best_model_name}): {final_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "I used GridSearchCV for all three models to systematically test key hyperparameters. For Random Forest, I tuned n_estimators, max_depth, and min_samples_split to balance bias, variance, and model complexity. For Gradient Boosting, I tuned n_estimators, max_depth, and learning_rate because these directly impact how the model fits residuals and how aggressively it learns patterns. Linear Regression has no meaningful hyperparameters to tune, so it remained a baseline. This strategy ensured each model had an optimized configuration without overfitting.\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "Yes. The log-transformed features and ratios worked especially well for tree-based models like Random Forest and Gradient Boosting, allowing them to capture subtle non-linear patterns. Linear Regression, on the other hand, benefited from the same feature engineering but showed that the data’s relationships were largely linear after preprocessing — which is why it ultimately achieved the lowest MAE on the test set despite being the simplest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model: Linear Regression\n",
      "Mean CV MAE: 60735.79\n",
      "Std Dev CV MAE: 4378.21\n",
      "Test MAE on Held-Out Test Set: 57664.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Choose the best model based on CV results (Linear Regression was identified as best)\n",
    "final_model_name = \"Linear Regression\"\n",
    "final_model = best_models[final_model_name]\n",
    "\n",
    "# Use the features that were selected for this model\n",
    "final_features = selected_feature_sets[final_model_name]\n",
    "\n",
    "# Cross-Validation on the final model\n",
    "X_selected = X_train[final_features]\n",
    "cv_scores = cross_val_score(final_model, X_selected, y_train, scoring=mae_scorer, cv=5)\n",
    "\n",
    "print(f\"Final Model: {final_model_name}\")\n",
    "print(f\"Mean CV MAE: {-np.mean(cv_scores):.2f}\")\n",
    "print(f\"Std Dev CV MAE: {np.std(cv_scores):.2f}\")\n",
    "\n",
    "# Retrain on full training set and evaluate on test set\n",
    "final_model.fit(X_selected, y_train)\n",
    "\n",
    "X_test_selected = X_test[final_features]\n",
    "test_preds = final_model.predict(X_test_selected)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "print(f\"Test MAE on Held-Out Test Set: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Selection\n",
    "I selected Linear Regression as my final model. Despite being the simplest algorithm tested, it delivered the lowest Mean Absolute Error (MAE) during cross-validation (≈60,735) and the best MAE on the held-out test set (≈57,664). Random Forest and Gradient Boosting were close contenders, but they didn’t outperform Linear Regression, even after tuning.\n",
    "The trade-off here was complexity versus performance: the tree-based models offered more flexibility and could capture non-linear patterns, but they didn’t yield better results. Linear Regression provided strong interpretability and reliability without overfitting, making it the most efficient choice.\n",
    "\n",
    "\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "Early in Milestone 1, I decided to log-transform skewed features like calculatedfinishedsquarefeet and lotsizesquarefeet and create engineered ratios such as sqft_per_lot. The goal was to normalize skewed data and give the models features with clearer relationships to the target variable.\n",
    "Looking back, this was a critical decision that improved model performance across the board. The log transformations stabilized variance and helped Linear Regression fit the data better, while also giving the tree-based models more balanced splits. I kept this preprocessing step because the validation scores confirmed that models performed worse when these features were untransformed, suggesting this engineering step was essential.\n",
    "\n",
    "\n",
    "\n",
    "3. Lessons Learned\n",
    "This project reinforced that good preprocessing and feature engineering often matter more than picking the fanciest model. Even though Random Forest and Gradient Boosting are more complex, they didn’t beat a well-prepared Linear Regression model because the data’s relationships were largely linear after transformation.\n",
    "If I had more time or data, I would explore interaction terms (e.g., bedrooms × bathrooms) and external datasets (such as neighborhood or economic indicators) to see if they could further improve predictive power. I’d also experiment with regularization techniques like Ridge or Lasso to test whether a lightly penalized linear model could reduce MAE even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
